---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Data Generation

```{r setup, include=F}
library(dplyr)
library(MASS)
library(mice)
library(VIM)
library(tidyverse)

set.seed(420)

n <- 1000
avg <- seq(0,2)
vcm <- matrix(data = c(1,1,1.7,1,2, 1.5, 1.7, 1.5, 4), nrow = 3, byrow = T)

data <- mvrnorm(1000, avg, vcm) %>% data.frame()
colnames(data) <- c("X", "Y", "W")

maxit_ <- 20 
m_ <- 5
method_ <- "midastouch" # Use midas touch as it is best for small donor pools
```

# task a)

The goal is to infer about the regression coefficient for X in the model $Y = \beta_0 + \beta_1X + \varepsilon , \varepsilon \sim N(0, \sigma^2)$. Set aside the first 900 observations as data from the main study and treat the remaining 100 observations as data from a substudy. Fit the above regression model on the main study data and store the point estimates of $\beta_0$, $\beta_1$ and $\sigma$ and the interval estimate of $\beta_1$.

```{r}
#Set aside first 900 obs as data from main study
main <- data %>% slice_head(n = 900)

#Remaining 100 observations as data from a substudy
sub <- data %>% slice_tail(n = 100)
```


```{r}
task_a <- function(main, sub){
  
  #Fit regression model on the main study 
  main.lm <- summary(lm(Y ~ X, data = main), conf.int=T)
  beta_0_main <- main.lm$coef[1,1]
  beta_0_sigma <- main.lm$coef[1,2]
  beta_1_main <- main.lm$coef[2,1]
  beta_1_sigma <- main.lm$coef[2,2]
  sigma_main <- main.lm$sigma
  
  int_est_beta_1 <- confint(lm(Y~X, data =main))[2,]
  
  ret <- list("beta_0" = beta_0_main,
              "beta_0_simga" = beta_0_sigma,
              "beta_1" = beta_1_main,
              "beta_1_sigma" = beta_1_sigma,
              "beta_1_confint" = as.numeric(int_est_beta_1),
              "sigma" = sigma_main)
  
  return(ret)
}

task_a(main, sub)
```

Extracting the $\sigma$ for the model is non-trivial for `mice` models. We have to do Rubin's rules by hand (this is taken from Ãœbung 4):

```{r}
MI_analysis <- function(Q_hat, U_hat, m) {
  #' Function that applies Rubin's rules to get the residual variance of the imputation
  if (class(Q_hat) == "matrix") {
    # pooled estimator:
    q_bar <- colSums(Q_hat) / m
    # within-variance:
    u_bar <- colSums(U_hat) / m
    # between-variance:
    B <-
      colSums((Q_hat - matrix(1, nrow = m, ncol = 1) %*% q_bar) ^ 2) / (m - 1)
  }
  else{
    q_bar <- sum(Q_hat) / m
    u_bar <- sum(U_hat) / m
    B <- (1 / (m - 1)) * sum((Q_hat - q_bar) ^ 2)
  }
  
  # total variance (sigma^2):
  sigma_sq <- u_bar + B + B / m
  
  # SD (sigma):
  sigma <- sqrt(sigma_sq)
  
  # degrees of freedom:
  df <- (m - 1) * (1 + (m / (m + 1)) * u_bar / B) ^ 2
  # confidence intervals:
  CIlow <- q_bar - qt(0.975, df) * sigma
  CIupper <- q_bar + qt(0.975, df) * sigma
  r <- (B + B / m) / u_bar
  
  return(list("sigma" = sigma, "squared" = sigma_sq, "CIl" = CIlow,  "CIu" = CIupper))
}

get_pred_var <- function(mids){
  mids_complete <- complete(mids, action = "long", include = F)
  
  theta_b <- aggregate(Y ~ .imp, data = mids_complete, mean)$Y
  var_b <- aggregate(Y ~ .imp, data = mids_complete, var)$Y / n
  
  analysis <- MI_analysis(theta_b, var_b, mids$m)
  
  return(analysis)
}

# Helper functions

gen_pred_mat <- function(){
  pred_mat <- matrix(rep(0, 9), ncol = 3)
  rownames(pred_mat) <- colnames(pred_mat) <- c("X", "Y", "W")
  
  return(pred_mat)
}

gen_relevant_vals <- function(summary, mids){
  
  ret <- list("beta_0" = summary$estimate[1],
              "beta_0_sigma" = summary$std.error[1],
              "beta_1" = summary$estimate[2],
              "beta_1_sigma" = summary$std.error[2],
              "beta_1_confint" = c(summary$`2.5 %`[2], summary$`97.5 %`[2]),
              "sigma" = get_pred_var(mids))
  
  return(ret)
}
```

# task b)

Create a data corresponding to scenario (a) in Figure 8.1 by deleting X from the main study. Multiply impute the missing values of X in the main study. Perform multiply imputed analysis and again store the point and interval estimates of the same parameters.

![Figure 8.1.](Figure_8.1.jpg)
## Computation of residual standard error $\sigma$ difficult for mice, use Rubin's rule


```{r}
task_b <- function(main, sub, method = method_, maxit = maxit_, m = m_){
  main["X"] <- NA
  
  # Immpute
  b_data <- rbind(main, sub)
  b_imp <- mice(b_data, print = F, maxit = maxit, m = m, method = method)
  
  # Plot diagnostics
  plot(b_imp)
  
  # Regress
  b_reg <- with(b_imp, lm(Y~X))
  b_res <- pool.syn(b_reg) # Use pool.syn as we have synthetic data!
  b_res_sum <- summary(b_res, conf.int = T)
  
  ret <- gen_relevant_vals(b_res_sum, b_imp)
  
  return(ret)
}

task_b(main, sub)
```

# task c)

Create a data corresponding to scenario (b) by deleting X values from the main study and Y values from the substudy. Multiply impute the missing values of X in the main study. Perform multiply imputed analysis as in (b).

> We tried a lot of things here. 

## First Try: Basic imputation strategy

Initial idea: Impute $f(Y|X,W)$ for 100 missing values, then impute $f(X|W,Y)$ for the remaining cases.

* See slide 194: It is suggested to include the DV into the imputation model
* Since `mice` only uses the non-imputed values for imputation by default, we can carry this out in one imputation

```{r}
task_c_basic <- function(main, sub, method = method_, maxit = maxit_, m = m_, exclude_y_for_x = F){
  main["X"] <- NA
  sub["Y"] <- NA
  
  c_data <- rbind(main, sub)
  
  # Predictor matrix
  c_pred_mat <- gen_pred_mat()
  
  c_pred_mat["X","W"] <- c_pred_mat["Y","X"] <- c_pred_mat["Y", "W"] <- 1
  if (!exclude_y_for_x) c_pred_mat["X","Y"] <- 1
  
  c_imp <- mice(c_data, print = F, maxit = maxit, m = m, predictorMatrix = c_pred_mat, method = method) 
  plot(c_imp)
  
  c_reg <- with(c_imp, lm(Y ~ X))
  c_res <- pool.syn(c_reg)
  c_res_sum <- summary(c_res, conf.int = T)
  
  ret <- gen_relevant_vals(c_res_sum, c_imp)
  
  return(ret)
}

task_c_basic(main, sub)
```

* Even though the results are not good, we can see that they are even worse when excluding $Y$ for the imputation of $X$

```{r}
task_c_basic(main, sub, exclude_y_for_x = T)
```

## Anothery try: Sequential regression

Another method would be to use sequential regression, where we iteratively solve the problem. We used slides 149 - 150 as a basis and used further literature to work this out:

* https://www.tandfonline.com/doi/full/10.1080/01621459.2014.948117

As a convergence criterion we use the squared difference between the current and the last $\beta_0, \beta_1$.  

```{r}
task_c_sequential <- function(main, sub, method = method_, maxit = maxit_, m = m_, thresh = 1e-09, max_iter = 100, print = T){
  
  main["X"] <- NA
  sub["Y"] <- NA
  
  c_sequential <- rbind(main, sub)

  # Initialize: Predict Y values via hot deck
  pred_map_seq <- gen_pred_mat()
  pred_map_seq["Y", "W"] <- 1
  
  init_mi <- mice(c_sequential, method = "sample", print = F, predictorMatrix = pred_map_seq)
  init_reg <- pool(with(init_mi, lm(Y~X)))
  
  # Fill estimated values with complete 
  c_sequential$Y <- complete(init_mi)$Y
  
  # Compute initial regression weights
  current_weights <- init_reg$pooled$estimate
  beta_0_archive <- c(current_weights[1])
  beta_1_archive <- c(current_weights[2])
  
  # Initialize old weights as arbitrary
  old_weights <- c(1,1)
  
  convergence_val <- sum((current_weights - old_weights)^2)
  
  pred_x <- pred_y <- gen_pred_mat()
  
  pred_x["X", "Y"] <- pred_x["X", "W"] <- 1
  pred_y["Y", "X"] <- pred_y["Y", "W"] <- 1
  
  n_runs <- 1
  while (n_runs <= max_iter & convergence_val > thresh){
    c_sequential$X <- complete(mice(c_sequential, method = method, m = m, maxit = maxit, printFlag = F, predictorMatrix = pred_x))$X
    c_sequential$Y <- complete(mice(c_sequential, method = method, m = m, maxit = maxit, printFlag = F, predictorMatrix = pred_y))$Y
    
    c_sequential_imp <- mice(c_sequential, method = method, m = m, maxit = maxit, printFlag = F)
    c_sequential_reg <- pool(with(c_sequential_imp, lm(Y~X)))
    
    old_weights <- current_weights
    
    current_weights <- c_sequential_reg$pooled$estimate
    
    convergence_val <- sum((current_weights - old_weights)^2)
    
    if (print) print(sprintf("Iteration %d: Convergence value %.6f", n_runs, convergence_val))

    n_runs <- n_runs + 1
  }
  
  c_sequential_res_sum <- summary(c_sequential_reg, conf.int = T)
  
  ret <- gen_relevant_vals(c_sequential_res_sum, c_sequential_imp)
  
  return(ret)
}

task_c_sequential(main, sub)
```

## Another try: CART

https://academic.oup.com/aje/article/172/9/1070/148540 uses sequential regression via CART. They show that it works well, so we decided to try it out too - but it does not lead to better results.

```{r}
task_c_basic(main, sub, method = "cart")
```

# task d)

Create a data corresponding to scenario (c) by deleting X on the main study, Y from the first 50 subjects in the substudy and W from the last 50 subjects. Perform multiply imputed analysis as in (b).

## First idea: Simply impute

Since we have some observations for each variable combination ($(X,Y), (Y,W), (X,W)$), we should be able to get good results through basic imputation.

```{r}
task_d_basic <- function(main, sub, method = method_, maxit = maxit_, m = m_){
  main["X"] <- NA
  sub[1:50, "Y"] <- NA
  sub[51:100, "W"] <- NA
  
  d_data <- rbind(main, sub)
  
  d_imp <- mice(d_data, print = F, maxit = maxit, m = m, method = method) 
  plot(d_imp)
  
  d_reg <- with(d_imp, lm(Y ~ X))
  d_res <- pool.syn(d_reg)
  d_res_sum <- summary(d_res, conf.int = T)
  
  ret <- gen_relevant_vals(d_res_sum, d_imp)
  
  return(ret)
}

task_d_basic(main, sub)
```

## Another idea: Sequentially complete sub, then main

Similar to what we did in C: First impute the missing values in sub, then impute the missing values in main. Steps: 

1. Use $n=900$ Y from main to impute $n = 50$ W in sub
2. Use W to impute $n = 50$ Y in sub and $n = 100$ X 
3. Use both to impute $n = 900$ X in main

```{r}
task_d_sequential <- function(main, sub, method = method_, maxit = maxit_, m = m_){
  main["X"] <- NA
  sub[1:50, "Y"] <- NA
  sub[51:100, "W"] <- NA
  
  d_data <- rbind(main, sub)
  
  # First step: Impute W based on Y
  pred_map_step_one <- gen_pred_mat()
  pred_map_step_one["W", "Y"] <- 1
  
  step_one <- mice(d_data, method = method, predictorMatrix = pred_map_step_one, m = m, maxit = maxit, printFlag = F)

  d_data$W <- complete(step_one)$W
  
  # Second step: Impute Y based on W, X
  pred_map_step_two <- gen_pred_mat()
  pred_map_step_two["Y", "W"] <- pred_map_step_two["Y", "X"] <- 1
  
  step_two <- mice(d_data, method = method, predictorMatrix = pred_map_step_two, m = m, maxit = maxit, printFlag = F)
  d_data$Y <- complete(step_two)$Y
  
  # Third step: Impute X based on W, Y
  pred_map_step_three <- gen_pred_mat()
  pred_map_step_three["X", "Y"] <- pred_map_step_three["X", "W"] <- 1
  
  step_three <- mice(d_data, method = method, predictorMatrix = pred_map_step_three, m = m, maxit = maxit, printFlag = F)
  d_data$X <- complete(step_three)$X
  
  d_reg <- with(step_three, lm(Y ~ X))
  d_res <- pool.syn(d_reg)
  d_res_sum <- summary(d_res, conf.int = T)
  
  ret <- gen_relevant_vals(d_res_sum, step_three)
  
  return(ret)
}

task_d_sequential(main, sub)
```

Surprisingly, this actually leads to way worse results.

# task e)

Generate new samples and repeat the process (a) to (d), 250 times.

```{r}
n <- 10
names <- c("a", "b", "c_1", "c_2", "c_3", "d_1", "d_2")

values <- vector(mode = "list", length = length(names))
names(values) <- names

for (i in 1:n){
  print(sprintf("Run %d", i))
  set.seed(i)
  
  values$a[[i]]   <- task_a(main, sub)
  values$b[[i]]   <- task_b(main, sub)
  values$c_1[[i]] <- task_c_basic(main, sub)
  values$c_2[[i]] <- task_c_sequential(main, sub, print = F)
  values$c_3[[i]] <- task_c_basic(main, sub, method = "cart")
  values$d_1[[i]] <- task_d_basic(main, sub)
  values$d_2[[i]] <- task_d_sequential(main, sub)
}
```

# task f)

Compare the bias and mean square properties of the estimates of $\beta_0$, $\beta_1$ and $\sigma^2$.

* **TO DO**: Mean square properties! (No idea yet)

* Bias: 

$$\frac{\text{est}-\text{true}}{\text{true}}$$

```{r}
bias <- function(est, true){
  return((est - true) / true)
}

bias_values_b0 <- bias_values_b1 <- data.frame(matrix(ncol = length(names)-1, nrow = n)) 
colnames(bias_values_b0) <- colnames(bias_values_b1) <- names[2:length(names)]

for (name in names[2:length(names)]){
  for (i in 1:n){
    bias_values_b0[i, name] <- bias(values[[name]][[i]]$beta_0, values[["a"]][[i]]$beta_0)
    bias_values_b1[i, name] <- bias(values[[name]][[i]]$beta_1, values[["a"]][[i]]$beta_1)
  }
}
```

We plotted the data: 

```{r}
bias_values_b0$beta <- "beta0"
bias_values_b1$beta <- "beta1"

bias_b0_long <- pivot_longer(bias_values_b0, cols = -c(beta))
bias_b1_long <- pivot_longer(bias_values_b1, cols = -c(beta))

bias_data <- rbind(bias_b0_long, bias_b1_long)

ggplot(bias_data, aes(x = name, y = value)) + 
  geom_boxplot() + 
  facet_wrap(~beta) +
  theme_minimal() + 
  ggtitle("Bias")
```


# task g)

* **TO DO**: We need to figure out whether we correctly computed the true value!

Compute the true value of $\beta_1$ and calculate the actual coverage rate for each method of estimating the confidence interval. Also, calculate the length of the confidence intervals.

* Coverage: Whether true value is within CI of our estimate


```{r}
coverage <- function(CI, true){
  return(as.numeric(CI[1] <= true & true <= CI[2]))
}

coverage_values <- data.frame(matrix(ncol = length(names)-1, nrow = n)) 
colnames(coverage_values) <- names[2:length(names)]

for (name in names[2:length(names)]){
  for (i in 1:n){
    coverage_values[i, name] <- coverage(values[[name]][[i]]$beta_1_confint, values[["a"]][[i]]$beta_1)
  }
}

coverage_values$index <- 1:n
coverage_long <- pivot_longer(coverage_values, cols = -c(index))

ggplot(data = coverage_long, aes(x = name, y = value/n)) +
  geom_bar(stat = "identity") + 
  xlab("method") +
  ylab("Density") + 
  theme_minimal()
```

